{
    "summary": "This code utilizes Google or Paddle OCR for text detection, removes noise and merges intersected texts, recognizes sentences, visualizes detections, saves results as JSON, and displays completion time.",
    "details": [
        {
            "comment": "This code contains two functions: `save_detection_json` and `visualize_texts`. The `save_detection_json` function takes a file path, list of texts, and image shape as inputs. It then creates an output dictionary containing the image shape and text information. For each text in the input list, it extracts the relevant data (id, content, location, width, height) and appends it to the 'texts' list within the output dictionary. Finally, it writes the output dictionary as a JSON file at the specified file path. The `visualize_texts` function takes an original image, a list of texts, an optional resize height, and an optional write path as inputs. It creates a copy of the original image and then visualizes each text within the image by calling the `visualize_element` method on each text object. If the `shown_resize_height` is specified, it resizes the image to that height while maintaining aspect ratio. Finally, if the `write_path` is provided, it writes the resulting image to a file at the specified path.",
            "location": "\"/media/root/Toshiba XG3/works/SingularGPT/docs/src/text_detection/text_detection.py\":0-33",
            "content": "import text_detection.ocr as ocr\nfrom text_detection.Text import Text\nimport numpy as np\nimport cv2\nimport json\nimport time\nimport os\nfrom os.path import join as pjoin\ndef save_detection_json(file_path, texts, img_shape):\n    f_out = open(file_path, 'w')\n    output = {'img_shape': img_shape, 'texts': []}\n    for text in texts:\n        c = {'id': text.id, 'content': text.content}\n        loc = text.location\n        c['column_min'], c['row_min'], c['column_max'], c['row_max'] = loc['left'], loc['top'], loc['right'], loc['bottom']\n        c['width'] = text.width\n        c['height'] = text.height\n        output['texts'].append(c)\n    json.dump(output, f_out, indent=4)\ndef visualize_texts(org_img, texts, shown_resize_height=None, write_path=None):\n    img = org_img.copy()\n    for text in texts:\n        text.visualize_element(img, line=2)\n    img_resize = img\n    if shown_resize_height is not None:\n        img_resize = cv2.resize(img, (int(shown_resize_height * (img.shape[1]/img.shape[0])), shown_resize_height))\n    if write_path is not None:"
        },
        {
            "comment": "This code contains two functions for text recognition: \"text_sentences_recognition\" and \"merge_intersected_texts\". The first function merges separate words detected by Google OCR into a sentence, while the second function merges intersected texts (sentences or words). Both functions use a while loop to continuously check for text merging opportunities until no more changes are made.",
            "location": "\"/media/root/Toshiba XG3/works/SingularGPT/docs/src/text_detection/text_detection.py\":34-71",
            "content": "        cv2.imwrite(write_path, img)\ndef text_sentences_recognition(texts):\n    '''\n    Merge separate words detected by Google ocr into a sentence\n    '''\n    changed = True\n    while changed:\n        changed = False\n        temp_set = []\n        for text_a in texts:\n            merged = False\n            for text_b in temp_set:\n                if text_a.is_on_same_line(text_b, 'h', bias_justify=0.2 * min(text_a.height, text_b.height), bias_gap=2 * max(text_a.word_width, text_b.word_width)):\n                    text_b.merge_text(text_a)\n                    merged = True\n                    changed = True\n                    break\n            if not merged:\n                temp_set.append(text_a)\n        texts = temp_set.copy()\n    for i, text in enumerate(texts):\n        text.id = i\n    return texts\ndef merge_intersected_texts(texts):\n    '''\n    Merge intersected texts (sentences or words)\n    '''\n    changed = True\n    while changed:\n        changed = False\n        temp_set = []\n        for text_a in texts:\n            merged = False"
        },
        {
            "comment": "The code processes OCR results, extracting text and coordinates from each result. It checks if the coordinates are valid and avoids processing any invalid results. If valid, it stores the text and its location in a list. The process continues for all OCR results.",
            "location": "\"/media/root/Toshiba XG3/works/SingularGPT/docs/src/text_detection/text_detection.py\":72-100",
            "content": "            for text_b in temp_set:\n                if text_a.is_intersected(text_b, bias=2):\n                    text_b.merge_text(text_a)\n                    merged = True\n                    changed = True\n                    break\n            if not merged:\n                temp_set.append(text_a)\n        texts = temp_set.copy()\n    return texts\ndef text_cvt_orc_format(ocr_result):\n    texts = []\n    if ocr_result is not None:\n        for i, result in enumerate(ocr_result):\n            error = False\n            x_coordinates = []\n            y_coordinates = []\n            text_location = result['boundingPoly']['vertices']\n            content = result['description']\n            for loc in text_location:\n                if 'x' not in loc or 'y' not in loc:\n                    error = True\n                    break\n                x_coordinates.append(loc['x'])\n                y_coordinates.append(loc['y'])\n            if error: continue\n            location = {'left': min(x_coordinates), 'top': min(y_coordinates),"
        },
        {
            "comment": "1. Function `text_detection` takes input file, output file, and optionally a paddle model for text detection.\n2. The method parameter specifies whether to use the Google or Paddle OCR method.\n3. Texts are detected using either Google's OCR API or Paddle OCR (if specified) and stored in a list.\n4. `text_cvt_orc_format_paddle` converts text detection results from Paddle OCR format to a standard format.\n5. `text_cvt_orc_format_paddle` extracts points, content, and location for each line of the text result.\n6. `text_filter_noise` filters out noisy or unnecessary texts from the list.",
            "location": "\"/media/root/Toshiba XG3/works/SingularGPT/docs/src/text_detection/text_detection.py\":101-136",
            "content": "                        'right': max(x_coordinates), 'bottom': max(y_coordinates)}\n            texts.append(Text(i, content, location))\n    return texts\ndef text_cvt_orc_format_paddle(paddle_result):\n    texts = []\n    result = paddle_result\n    _ls = []\n    for i in result:\n      for j in i:\n        _ls.append(j)\n    for i, line in enumerate(_ls):\n        points = np.array(line[0])\n        location = {'left': int(min(points[:, 0])), 'top': int(min(points[:, 1])), 'right': int(max(points[:, 0])),\n                    'bottom': int(max(points[:, 1]))}\n        content = line[1][0]\n        texts.append(Text(i, content, location))\n    return texts\ndef text_filter_noise(texts):\n    valid_texts = []\n    for text in texts:\n        if len(text.content) <= 1 and text.content.lower() not in ['a', ',', '.', '!', '?', '$', '%', ':', '&', '+']:\n            continue\n        valid_texts.append(text)\n    return valid_texts\ndef text_detection(input_file, output_file, method='paddle', paddle_model=None):\n    '''\n    :param method: google or paddle"
        },
        {
            "comment": "This code is for text detection using either Google OCR or Paddle OCR, and it takes an input file as a parameter. It reads the image from the input file, and depending on the method selected, it performs text detection using either Google's OCR API or PaddleOCR library. The results are processed further to remove noise, merge intersected texts, and recognize sentences before storing them in the specified output folder.",
            "location": "\"/media/root/Toshiba XG3/works/SingularGPT/docs/src/text_detection/text_detection.py\":137-160",
            "content": "    :param paddle_model: the preload paddle model for paddle ocr\n    '''\n    start = time.process_time()\n    name = input_file.split('/')[-1][:-4]\n    ocr_root = pjoin(output_file, 'ocr')\n    img = cv2.imread(input_file)\n    if method == 'google':\n        print('*** Detect Text through Google OCR ***')\n        ocr_result = ocr.ocr_detection_google(input_file)\n        texts = text_cvt_orc_format(ocr_result)\n        texts = merge_intersected_texts(texts)\n        texts = text_filter_noise(texts)\n        texts = text_sentences_recognition(texts)\n    elif method == 'paddle':\n        # The import of the paddle ocr can be separate to the beginning of the program if you decide to use this method\n        from paddleocr import PaddleOCR\n        print('*** Detect Text through Paddle OCR ***')\n        if paddle_model is None:\n            paddle_model = PaddleOCR(use_angle_cls=True, lang=\"en\")\n        result = paddle_model.ocr(input_file, cls=True)\n        texts = text_cvt_orc_format_paddle(result)\n    else:\n        raise ValueError('Method has to be \"google\" or \"paddle\"')"
        },
        {
            "comment": "This code snippet visualizes detected texts, saves the detection results as JSON and calculates elapsed time of the process. It then prints the completion time and output file path.",
            "location": "\"/media/root/Toshiba XG3/works/SingularGPT/docs/src/text_detection/text_detection.py\":162-167",
            "content": "    visualize_texts(img, texts, shown_resize_height=800, write_path=pjoin(ocr_root, name+'.png'))\n    save_detection_json(pjoin(ocr_root, name+'.json'), texts, img.shape)\n    # calculate the elapsed time of the compositional detection process\n    elapsed_time = time.process_time() - start\n    print(f\"[Text Detection Completed in {elapsed_time:.3f} seconds.]\\nOutput JSON file path: {pjoin(ocr_root, name + '.json')}\")"
        }
    ]
}